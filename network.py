import numpy as np
import random

class SequentialNetwork:
    def __init__(self, loss=None):
        print("Initialize Network...")
        self.layers = []
        if loss is None:
            self.loss = MSE()
    def add(self, layer):
        self.layers.append(layer)
        layer.describe()
        if len(self.layers) > 1:
            self.layers[-1].connect(self.layers[-2])
    def train(self, training_data, epochs, mini_batch_size,
        learning_rate, test_data=None):
        n = len(training_data)
        for epoch in range(epochs):
            random.shuffle(training_data)
            mini_batches = [
            training_data[k:k + mini_batch_size] for
            k in range(0, n, mini_batch_size)
            ]
        for mini_batch in mini_batches:
            self.train_batch(mini_batch, learning_rate)
        if test_data:
            n_test = len(test_data)
            print("Epoch {0}: {1} / {2}"
            .format(epoch, self.evaluate(test_data), n_test))
        else:
            print("Epoch {0} complete".format(epoch))
    def train_batch(self, mini_batch, learning_rate):
        self.forward_backward(mini_batch)
        self.update(mini_batch, learning_rate) 
    def update(self, mini_batch, learning_rate):
        learning_rate = learning_rate / len(mini_batch)
        for layer in self.layers:
            layer.update_params(learning_rate)
        for layer in self.layers:
            layer.clear_deltas()
    def forward_backward(self, mini_batch):
        for x, y in mini_batch:
            self.layers[0].input_data = x
        for layer in self.layers:
            layer.forward()
            self.layers[-1].input_delta = \
            self.loss.loss_derivative(self.layers[-1].output_data, y)
        for layer in reversed(self.layers):
            layer.backward() 
    def single_forward(self, x):
        self.layers[0].input_data = x
        for layer in self.layers:
            layer.forward()
        return self.layers[-1].output_data
    def evaluate(self, test_data):
        test_results = [(
        np.argmax(self.single_forward(x)),
        np.argmax(y)
        ) for (x, y) in test_data]
        return sum(int(x == y) for (x, y) in test_results)

class MSE:                                    
    def __init__(self):
        pass
    @staticmethod
    def loss_function(predictions, labels):
        diff = predictions - labels
        return 0.5 * sum(diff * diff)[0]        
    @staticmethod
    def loss_derivative(predictions, labels):
        print(predictions, labels)
        return predictions - labels

class Layer:
    def __init__(self):
        self.params = []
        self.previous = None
        self.next = None
        self.input_data = None
        self.output_data = None
        self.input_delta = None
        self.output_delta = None

    def connect(self, layer):
        self.previous = layer
        layer.next = self
        
    def forward(self):
        data = self.get_forward_input()
        self.output_data = np.dot(self.weight, data) + self.bias 
        
    def get_forward_input(self):
        if self.previous is not None:
            return self.previous.output_data
        else:
            return self.input_data
             
    def backward(self):
        data = self.get_forward_input()
        delta = self.get_backward_input()
        self.delta_b += delta
        self.delta_w += np.dot(delta, data.transpose())
        self.output_delta = np.dot(self.weight.transpose(), delta)  
        
    def get_backward_input(self):
        if self.next is not None:
            return self.next.output_delta
        else:
            return self.input_delta
            
    def update_params(self, rate):
        self.weight -= rate * self.delta_w
        self.bias -= rate * self.delta_b
        
    def clear_deltas(self):
        self.delta_w = np.zeros(self.weight.shape)
        self.delta_b = np.zeros(self.bias.shape)
        
    def describe(self):
        print("|--- " + self.__class__.__name__)
        print(" |-- dimensions: ({},{})"
        .format(self.input_dim, self.output_dim))    

class ActivationLayer(Layer):
    def __init__(self, input_dim):
        super(ActivationLayer, self).__init__()
        self.input_dim = input_dim
        self.output_dim = input_dim
    def forward(self):
        data = self.get_forward_input()
        self.output_data = sigmoid(data)
    def backward(self):
        delta = self.get_backward_input()
        data = self.get_forward_input()
        self.output_delta = delta * sigmoid_prime(data)
    def describe(self):
        print("|-- " + self.__class__.__name__)
        print(" |-- dimensions: ({},{})"
        .format(self.input_dim, self.output_dim))

class DenseLayer(Layer):
    def __init__(self, input_dim, output_dim):
        super(DenseLayer, self).__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.weight = np.random.randn(output_dim, input_dim)
        self.bias = np.random.randn(output_dim, 1)
        self.params = [self.weight, self.bias]
        self.delta_w = np.zeros(self.weight.shape)
        self.delta_b = np.zeros(self.bias.shape)




